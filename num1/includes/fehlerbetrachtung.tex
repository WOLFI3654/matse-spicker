\section{Fehlerbetrachtung}

\begin{defi}{Gut gestelltes Problem}
    Praktisch alle Aufgabenstellungen der numerischen Mathematik lassen sich als (eventuell sehr kompliziertes) Nullstellenproblem schreiben:
    \begin{itemize}
        \item gegeben ist eine Funktion $g$ und Eingabedaten $x$
        \item suche $y$ mit $g(x, y) = 0$
    \end{itemize}

    Folgende Eigenschaften sollte ein solches Problem sinnvollerweise erfüllen:
    \begin{itemize}
        \item \emph{Existenz}: es sollte Lösungen geben
        \item \emph{Eindeutigkeit}: zu jedem $x$ sollte es genau ein $y$ geben
        \item \emph{Stetige Abhängigkeit}:\footnote{Stetige Abhängigkeit garantiert, dass (hinreichend) kleine Störungen der Eingabedaten nur kleine Veränderungen der Ergebnisse verursachen.} der Lösung $y$ von den Eingabedaten $x$, d. h.
              \[
                  x' \to x \quad \implies \quad y' = f(x') \to f(x) = y
              \]
    \end{itemize}

    Gilt für das Problem $g(x, y) = 0$ Existenz, Eindeutigkeit und stetige Abhängigkeit, so nennt man das Problem \emph{gut gestellt} (\enquote{well posed}), ansonsten \emph{schlecht gestellt} (\enquote{ill posed}).

    Müssen schlecht gestellte Probleme behandelt werden, so approximiert man sie in der Regel durch gut gestellte Ersatzprobleme (Regularisierung), wobei die Approximationseigenschaften genau kontrolliert werden müssen.
\end{defi}

\begin{example}{Gut gestelltes Problem}

    \begin{itemize}
        \item Ein lineares Gleichungssystem $Ax = b$ ist äquivalent zu:
              \[
                  g(x, y) = 0, \quad x = (A, b), \quad g(x, y) = Ay - b
              \]

              Wir erhalten:
              \[
                  y = f(x) = A^{-1} b, \quad x = (A, b), \quad \det(A) \neq 0
              \]
        \item Die größere der beiden Nullstellen von $t^2 + 2pt - q = 0$, $p, q > 0$ ist gegeben durch:
              \[
                  t_2 = -p + \sqrt{p^2 + q}
              \]
              d. h. das Nullstellenproblem ist äquivalent zu:
              \[
                  g(x, y) = 0, \quad x = (p, q), \quad g(x, y) = y - (-p + \sqrt{p^2 + q})
              \]

              Wir erhalten:
              \[
                  y = f(x) = -p + \sqrt{p^2 + q}, \quad x = (p, q)^T, \quad p, q > 0
              \]
    \end{itemize}

    Beide Funktionen sind stetig, so dass beide Probleme gut gestellt sind.
\end{example}

\subsection{Fehlergrößen}

\begin{defi}{Fehler}
    Sei ein gut gestelltes Problem
    \[
        y = f(x), \quad f: X \to Y
    \]
    gegeben mit:
    \begin{itemize}
        \item $x$ bekannt
        \item $X$ und $Y$ sind Vektorräume mit Normen $\| \cdot \|_X$ bzw. $\| \cdot \|_Y$
    \end{itemize}

    Zur numerischen Behandlung wird $f$ durch ein (diskretes, endliches) Problem $\hat{f}$ ersetzt, das anschließend in Floating-Point-Arithmetik als $\tilde{f}$ implementiert wird.

    Wir kennen also:
    \begin{itemize}
        \item $f(x) = y$:
              \begin{itemize}
                  \item $x$ sind \emph{exakte Eingabedaten}
                  \item $f$ ist ein \emph{gut gestelltes Problem}
                  \item $y$ ist die \emph{exakte Lösung}
              \end{itemize}
        \item $\hat{f}(x) = \hat{y}$:
              \begin{itemize}
                  \item $x$ sind exakte Eingabedaten
                  \item $\hat{f}$ ist das \emph{Ersatzproblem}
                  \item $\hat{y}$ ist die \emph{exakte Lösung des Ersatzproblems}
              \end{itemize}
        \item $\tilde{f}(\tilde{x}) = \tilde{y}$:
              \begin{itemize}
                  \item $\tilde{x}$ sind Eingabedaten in \emph{Floating-Point-Darstellung}
                  \item $\tilde{f}$ ist die \emph{Floating-Point-Implementierung des Ersatzproblems}
                  \item $\tilde{y}$ ist das \emph{Ergebnis des in Floating-Point-Arithmetik durchgeführten Ersatzproblems}
              \end{itemize}
    \end{itemize}

    Die Qualität eines Approximationsverfahrens wird dadurch bestimmt, wie weit $\tilde{y}$ von $y$ abweicht.
    Um diese Abweichung quantitativ zu fassen, werden die folgenden Fehlergrößen definiert:
    \begin{itemize}
        \item \emph{Fehler}:
              \[
                  e := \tilde{y} - y
              \]
        \item \emph{absoluter Fehler}:
              \[
                  e_a := \| \tilde{y} - y \|_Y
              \]
        \item \emph{relativer Fehler}:
              \[
                  e_r := \frac{\| \tilde{y} - y \|_Y}{\| y \|_Y}, \quad \| y \|_Y \neq 0
              \]
    \end{itemize}
\end{defi}

\begin{defi}{Gemischtes Fehlerkriterium}
    In numerischen Verfahren versucht man den Fehler zu kontrollieren und dabei (wenn möglich) unter eine
    vom Benutzer vorgegebene Schranke $\epsilon$ zu drücken.

    Der relative Fehler ist eigentlich aussagekräftiger, kann aber bei kleinen Absolutwerten von $y$ eine unnötig hohe Genauigkeit erzwingen.
    Deshalb benutzt man in der Praxis häufig ein Kriterium, das beide Fehlerarten kombiniert.

    Seien $\epsilon_a$ und $\epsilon_r$ gegebene Schranken für den absoluten bzw. relativen Fehler.

    Beim \emph{gemischten Fehlerkriterium} versucht man eine Approximation $\tilde{y}$ von $y$ zu finden, mit
    \[
        \| \tilde{y} - y \|_Y \leq \epsilon_a + \| y \|_Y \epsilon_r
    \]

    Es gilt:
    \begin{itemize}
        \item Ist der exakte Absolutwert $\| y \|_Y$ sehr klein, dann dominiert $\epsilon_a$ und
              \[
                  e_a = \| \tilde{y} - y \|_Y \lesssim \epsilon_a
              \]
        \item Ist der exakte Absolutwert $\| y \|_Y$ sehr groß, dann dominiert $\|y \|_Y \epsilon_r$ und
              \[
                  \| \tilde{y} - y \|_Y \lesssim \|y \|_Y \epsilon_r \quad \iff \quad e_r = \frac{\| \tilde{y} - y \|_Y}{\| y \|_Y} \lesssim \epsilon_r
              \]
    \end{itemize}
\end{defi}

\subsection{Fehlerquellen}

\begin{defi}{Eingabefehler}
    Wegen der Umwandlung der Eingabedaten $x$ in Floating-Point-Darstellung $\tilde{x}$ tritt der \emph{Eingabefehler} immer auf und wird deshalb auch als \emph{unvermeidbarer Fehler} bezeichnet:
    \[
        f(\tilde{x}) - f(x)
    \]

    Die Größe dieses Fehleranteils hängt davon ab, wie empfindlich das Originalproblem $f$ auf Störungen in den Eingabedaten reagiert.
\end{defi}

\begin{defi}{Verfahrensfehler}
    Beim \emph{Verfahrensfehler} wird das in exakter Arithmetik gerechnete Ersatzproblem $\hat{f}$ mit dem Originalproblem $f$ für den selben Input $\tilde{x}$ verglichen:
    \[
        \hat{f}(\tilde{x}) - f(\tilde{x})
    \]

    Die Größe dieses Fehleranteils hängt von der Qualität des Ersatzproblems ab.
\end{defi}

\begin{defi}{Akkumulierter Rundungsfehler}
    Das Ersatzproblem $\hat{f}$ besteht aus einer endlichen Anzahl von Elementaroperationen (Additionen, Multiplikationen etc.), die bei der Floating-Point-Implementierung $\tilde{f}$ durch ihre Floating-Point-Äquivalente ersetzt werden.

    Jede Floating-Point-Operation erzeugt in der Regel Rundungsfehler, die sich akkumulieren.

    Die Größe dieses Fehleranteils hängt davon ab, wie robust das Ersatzproblem auf eine Floating-Point-Implementierung reagiert:
    \[
        \tilde{f}(\tilde{x}) - \hat{f}(\tilde{x})
    \]
\end{defi}

\begin{defi}{Fließkommazahl}
    Eine \emph{Fließkommazahl} zur Basis $b$ hat die Darstellung
    \[
        v \cdot m \cdot b^e
    \]
    Dabei ist:
    \begin{itemize}
        \item $b \in \N$ die \emph{Basis}
        \item $v = \pm 1$ das \emph{Vorzeichen}
        \item $m$ die \emph{Mantisse}, ein $b$-adischer Bruch
              \[
                  m = m_{-k} \ldots m_{-1}m_0 . m_1 \ldots m_l \quad \iff \quad m = \sum_{i = -k}^{l} m_i \cdot b^{-i}
              \]
              mit Ziffern $m_i \in \{ 0, 1, \ldots, b-1 \}$ und Stellenzahl $k + l + 1$
        \item $e \in \Z$ der \emph{Exponent}
    \end{itemize}

    In dieser Form sind Fließkommadarstellungen einer Zahl nicht eindeutig, denn durch Änderung des Exponenten erhält man z. B.
    \begin{itemize}
        \item $\num{-43210000}$
        \item $-43.21 \cdot 10^6$
        \item $-4.321 \cdot 10^7$
    \end{itemize}

    Die \emph{normalisierte Darstellung}
    \[
        m = \pm (m_0 . m_1 \ldots m_l) \cdot b^e \quad \iff \quad m = \pm \left( \sum_{i = 0}^{l} m_i b^{-i} \right) \cdot b^e, \quad m_0 \neq 0
    \]
    ist für Zahlen $\neq 0$ eindeutig.
\end{defi}

\begin{defi}{Maschinengenauigkeit}
    Jede reelle Zahl $x \in \R$ kann als normalisierte Fließkommazahl zur Basis $b$ mit unendlich langer Mantisse dargestellt werden.

    Durch Runden erzeugt man aus $x$ eine Fließkommazahl $\tilde{x}$ mit Stellenzahl $l + 1$.
    Dazu wird in der Regel dasjenige $\tilde{x}$ ausgewählt, das $x$ am nächsten liegt (\emph{nearest Rundung}.)

    Betrachten wir normierte Fließkommazahlen zur Basis $b$ mit Mantissenlänge $l$, so bezeichnet
    \[
        \epsilon_M := \frac{1}{2} b^{-l}
    \]
    die \emph{Maschinengenauigkeit}.

    Es gilt:
    \begin{itemize}
        \item $\epsilon_M$ ist der maximale relative Fehler, der bei der Umwandlung einer rellen Zahl (bei nearest Rundung) entstehen kann.
        \item $\epsilon_M$ hängt nur von der Basis und der Mantissenlänge, nicht aber vom Exponenten ab.
    \end{itemize}
\end{defi}

\begin{defi}{Kondition}
    Sei $f: X \to Y$.
    Die kleinsten Konstanten $\varkappa_a$, $\varkappa_r$ mit
    \[
        \| f(x') - f(x) \|_Y \leq \varkappa_a \| x' - x \|_X
    \]
    \[
        \frac{\| f(x') - f(x) \|_Y}{\| f(x) \|_Y} \leq \varkappa_r \frac{\| x' - x \|_X}{\| x \|_X}
    \]
    und
    \[
        \| x' - x \|_X \leq \delta
    \]
    heißen \emph{absolute Kondition} bzw. \emph{relative Kondition} von $f$ in $x$ und hängen von den benutzten Normen, $x$ und $\delta$ ab.

    Die Konditionszahlen beschreiben, wie stark $f$ Störungen in $x$ schlimmstenfalls verstärkt.

    Kleine Konditionszahlen deuten auf ein gut konditioniertes, große auf ein schlecht konditioniertes Problem hin.

    Werden die Störungen in $x$ durch die Umwandlung in ein Floating-Point-Format verursacht, d.h. $x' =  \tilde{x}$, so gilt:
    \[
        e_r(x) = \frac{\| \tilde{x} - x \|_X}{\| x \|_X} \leq \epsilon_M
    \]
    bzw.
    \[
        e_r(y) = \frac{\| \tilde{y} - y \|_Y}{\| y \|_Y} \leq \varkappa_r \frac{\| \tilde{x} - x \|_X}{\| x \|_X} \leq \varkappa_r \epsilon_M
    \]
    also
    \[
        e_r(y) \quad \text{falls} \quad \varkappa_r \leq \frac{\epsilon_r}{\epsilon_M}
    \]
    d. h. eine sinnvolle Berechnung von $f(x)$ (relativer Fehler unterhalb $\epsilon_r$) ist nur möglich, wenn die Kondition $\varkappa_r$ in Abhängigkeit von der Maschinengenauigkeit $\epsilon_M$ eine gewisse Grenze nicht überschreitet.
\end{defi}

\begin{bonus}{Konditionszahlen herleiten}
    Für stetig differenzierbares $f$ kann man relativ einfach obere Schranken für die Konditionszahlen herleiten.
    Wir betrachten zunächst den einfachen Fall $f: \R \to \R$.

    Nach dem Mittelwertsatz gilt:
    \[
        f(x') = f(x) + f'(\zeta)(x' - x), \quad \zeta \in [ \min(x, x') , \max(x, x') ]
    \]
    und damit:
    \[
        | f(x') - f(x) | = | f'(\zeta) | | x' - x | \leq \max_{\zeta \in [ \min(x, x') , \max(x, x') ]} | f'(\zeta) | | x' - x |
    \]

    Für alle $x'$ mit $| x' - x | \leq \delta$ gilt:
    \[
        [ \min(x, x') , \max(x, x') ] \in [ x - \delta, x + \delta ]
    \]
    und somit:
    \[
        \zeta \in [ \min(x, x') , \max(x, x') ] \quad \implies \quad | \zeta - x | \leq \delta
    \]

    Damit erhalten wir schließlich die Abschätzungen:
    \[
        \varkappa_a \leq \max_{| \zeta - x | \leq \delta} | f'(\zeta) |
    \]
    \[
        \varkappa_r \leq \frac{| x |}{| f(x) |} \max_{| \zeta - x | \leq \delta} | f'(\zeta) |
    \]
\end{bonus}

\begin{defi}{Fehlerfortpflanzungsformeln der differentiellen Fehleranalyse}
    TODO
\end{defi}